{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = 'whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Mereka&amp;quot; ini adalah contoh Makhluk&amp;quot; y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Ini sdh mjd pilihan mreka, manusia mn pun gak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>bisa diliat ini mereka berdua open minded bang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Aku seneng kalau melihat orang senang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>suka sama podcast nyaðŸ˜„ðŸ˜„ðŸ˜„&lt;br&gt;gak bosen-bosen no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Gua ragu,gua gatau deh bingung au ah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Jadi aku pernah iseng2 nih nyari temen onlen g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>2.0</td>\n",
       "      <td>mungkin kalo ga kenal nct gw udh jadi lesbian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Gua bukan LGBT kak, tapi gak sedikit temen gua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Gue juga punya temen kerja Gay , gue tau dia g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Review                                          Sentiment\n",
       "0        0.0  Mereka&quot; ini adalah contoh Makhluk&quot; y...\n",
       "1        1.0  Ini sdh mjd pilihan mreka, manusia mn pun gak ...\n",
       "2        1.0  bisa diliat ini mereka berdua open minded bang...\n",
       "3        2.0              Aku seneng kalau melihat orang senang\n",
       "4        2.0  suka sama podcast nyaðŸ˜„ðŸ˜„ðŸ˜„<br>gak bosen-bosen no...\n",
       "...      ...                                                ...\n",
       "3122     2.0               Gua ragu,gua gatau deh bingung au ah\n",
       "3123     1.0  Jadi aku pernah iseng2 nih nyari temen onlen g...\n",
       "3124     2.0  mungkin kalo ga kenal nct gw udh jadi lesbian ...\n",
       "3125     1.0  Gua bukan LGBT kak, tapi gak sedikit temen gua...\n",
       "3126     2.0  Gue juga punya temen kerja Gay , gue tau dia g...\n",
       "\n",
       "[3127 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('lgbt_3.csv',header=None,skiprows=1,names=['Review','Sentiment'], dtype={'text': \"string\"})\n",
    "X = data.Review\n",
    "y = data.Sentiment\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-239604d69692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTAG_CLEANING_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@\\S+\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Remove @tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAG_CLEANING_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "TAG_CLEANING_RE = \"@\\S+\"\n",
    "# Remove @tags\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TAG_CLEANING_RE, ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "    return text\n",
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text) \n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered \n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing tweets data\n",
    "data['clean'] = data['Review'].apply(cleaningText)\n",
    "# tweets['text_clean'] = tweets['tweet'].apply(cleaningText)\n",
    "# tweets['text_clean'] = tweets['text_clean'].apply(casefoldingText)\n",
    "# tweets.drop(['tweet'], axis = 1, inplace = True)\n",
    "\n",
    "# tweets['text_preprocessed'] = tweets['text_clean'].apply(tokenizingText)\n",
    "# tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(filteringText)\n",
    "# tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(stemmingText)\n",
    "\n",
    "# # drop duplicates/spams tweets\n",
    "# tweets.drop_duplicates(subset = 'text_clean', inplace = True)\n",
    "\n",
    "# Export to csv file\n",
    "# tweets.to_csv(r'25k_tweets_data_clean.csv', index = False, header = True,index_label=None)\n",
    "\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy using Naive Bayes Model\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train_dtm, y_train)\n",
    "#y_pred = NB.predict(X_test_dtm)\n",
    "#print('\\nNaive Bayes')\n",
    "#print('Accuracy Score: ',metrics.accuracy_score(y_test,y_pred)*100,'%',sep='')\n",
    "#print('Confusion Matrix: ',metrics.confusion_matrix(y_test,y_pred), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
